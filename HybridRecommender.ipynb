{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "content_model = joblib.load(\"content_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_matrix = np.load(\"feature_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\Projects\\\\Machine Learning\\\\Music Recommender System\\\\Music Info.csv\")\n",
    "df2 = pd.read_csv(\"E:\\\\Projects\\\\Machine Learning\\\\Music Recommender System\\\\User Listening History.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load mappings from saved JSON files\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "user_to_index = load_json(\"user_to_index.json\")\n",
    "track_to_index = load_json(\"track_to_index.json\")\n",
    "index_to_track = load_json(\"index_to_track.json\")\n",
    "track_to_name = load_json(\"track_to_name.json\")\n",
    "\n",
    "print(\"Mappings loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_embedding): Embedding(962037, 32)\n",
       "  (item_embedding): Embedding(50683, 32)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embeds = self.user_embedding(user_ids)\n",
    "        item_embeds = self.item_embedding(item_ids)\n",
    "        x = torch.cat([user_embeds, item_embeds], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "num_users = len(user_to_index)\n",
    "num_items = len(track_to_index)  \n",
    "\n",
    "collaborative_model = NCF(num_users, num_items)  \n",
    "state_dict = torch.load(\"collaborative_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "\n",
    "old_num_items = state_dict['item_embedding.weight'].shape[0]\n",
    "new_num_items = num_items\n",
    "\n",
    "if old_num_items < new_num_items:\n",
    "    # Expand the embedding matrix\n",
    "    new_embedding = torch.nn.Embedding(new_num_items, collaborative_model.item_embedding.embedding_dim)\n",
    "    new_embedding.weight.data[:old_num_items] = state_dict['item_embedding.weight']\n",
    "    state_dict['item_embedding.weight'] = new_embedding.weight\n",
    "\n",
    "collaborative_model.load_state_dict(state_dict, strict=False)\n",
    "collaborative_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_recommend_by_name(song_name, df, feature_matrix, knn):\n",
    "\n",
    "    try:\n",
    "        idx = df[df[\"name\"] == song_name].index[0]  \n",
    "        distances, indices = knn.kneighbors([feature_matrix[idx]])\n",
    "        recommended_track_ids = df.iloc[indices[0][1:]][\"track_id\"].tolist()\n",
    "        recommended_song_names = df[df[\"track_id\"].isin(recommended_track_ids)][\"name\"].tolist()\n",
    "        return recommended_song_names\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Song '{song_name}' not found in the dataset.\")\n",
    "        return None\n",
    "    except KeyError:\n",
    "        print(\"Required columns 'name' or 'track_id' not found in DataFrame\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_hybrid(user_id, song_name, top_n=10):\n",
    "    if user_id not in user_to_index:\n",
    "        print(f\"User ID {user_id} not found, using content-based filtering.\")\n",
    "        return knn_recommend_by_name(song_name, df, feature_matrix, content_model)\n",
    "\n",
    "    user_idx = user_to_index.get(user_id, -1)\n",
    "\n",
    "    # Ensure user_id is valid\n",
    "    if user_idx == -1 or user_idx >= collaborative_model.user_embedding.num_embeddings:\n",
    "        print(f\"User ID {user_id} out of range, using content-based filtering.\")\n",
    "        return knn_recommend_by_name(song_name, df, feature_matrix, content_model)\n",
    "\n",
    "    user_tensor = torch.tensor([user_idx] * len(track_to_index), dtype=torch.long)\n",
    "    track_tensor = torch.tensor(list(track_to_index.values()), dtype=torch.long)\n",
    "\n",
    "    # Clip indices if out of range\n",
    "    user_tensor = torch.clamp(user_tensor, max=collaborative_model.user_embedding.num_embeddings - 1)\n",
    "    track_tensor = torch.clamp(track_tensor, max=collaborative_model.item_embedding.num_embeddings - 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = collaborative_model(user_tensor, track_tensor)\n",
    "\n",
    "    _, indices = torch.topk(predictions, top_n)\n",
    "    collab_recommended_ids = [index_to_track[str(idx.item())] for idx in indices]\n",
    "\n",
    "    # Convert track IDs to song names\n",
    "    collab_recommended_songs = [track_to_name.get(track_id, \"Unknown Song\") for track_id in collab_recommended_ids]\n",
    "\n",
    "    # Content-based filtering recommendations\n",
    "    content_recommended_songs = knn_recommend_by_name(song_name, df, feature_matrix, content_model)\n",
    "\n",
    "    # Hybrid Approach: Combine Results\n",
    "    hybrid_recommendations = list(set(collab_recommended_songs + content_recommended_songs))\n",
    "\n",
    "    return hybrid_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few track indices: [('b80344d063b5ccb3212f76538f3d9e43d87dca9e', 0), ('85c1f87fea955d09b4bec2e36aee110927aedf9a', 1), ('bd4c6e843f00bd476847fb75c47b4fb430a06856', 2), ('969cc6fb74e076a68e36a04409cb9d3765757508', 3), ('4bd88bfb25263a75bbdd467e74018f4ae570e5df', 4), ('e006b1a48f466bf59feefed32bec6494495a4436', 5), ('9d6f0ead607ac2a6c2460e4d14fb439a146b7dec', 6), ('9bb911319fbc04f01755814cb5edb21df3d1a336', 7), ('b64cdd1a0bd907e5e00b39e345194768e330d652', 8), ('17aa9f6dbdf753831da8f38c71b66b64373de613', 9)]\n",
      "Last few track indices: [('TROIHJK12903CECC08', 50673), ('TRMEHFD128F92E4557', 50674), ('TRXWSIN128F9339A11', 50675), ('TRONQMR12903CF533E', 50676), ('TRPIGDW12903CDEB2D', 50677), ('TRQYCFV128F9322F50', 50678), ('TRHQCSH128F42724B7', 50679), ('TRZRODK128F92D68D7', 50680), ('TRGLMEM128F9322F63', 50681), ('TRIPFKO128F42383FE', 50682)]\n"
     ]
    }
   ],
   "source": [
    "print(\"First few track indices:\", list(user_to_index.items())[:10])\n",
    "print(\"Last few track indices:\", list(track_to_index.items())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tracks Indexed: 962037\n",
      "Example Mapped Track IDs: ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', '85c1f87fea955d09b4bec2e36aee110927aedf9a', 'bd4c6e843f00bd476847fb75c47b4fb430a06856', '969cc6fb74e076a68e36a04409cb9d3765757508', '4bd88bfb25263a75bbdd467e74018f4ae570e5df']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tracks Indexed: {len(user_to_index)}\")\n",
    "print(f\"Example Mapped Track IDs: {list(user_to_index.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations: [\"Somebody Else's Guy\", 'Consider Us Dead', 'Pattern Recognition', \"Somebody's Me\", 'Den Svarta Fanan', 'The Thespian', 'These Are The Days', 'My Little Brother', 'Juicebox', 'Relieved Beyond Repair', 'We the People', 'Another one goes by', 'Living Life', 'Like a Prayer']\n"
     ]
    }
   ],
   "source": [
    "user_id = 'b80344d063b5ccb3212f76538f3d9e43d87dca9e'\n",
    "song_name = \"Nothing From Nothing\"\n",
    "\n",
    "recommendations = recommend_songs_hybrid(user_id, song_name)\n",
    "print(\"Hybrid Recommendations:\", recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
